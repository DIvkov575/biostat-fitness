{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T16:56:21.203998Z",
     "start_time": "2025-09-05T16:56:18.150376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import kendalltau\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import train_test_split\n"
   ],
   "id": "71f4035743ddaa6c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/biostat-fitness/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm, gaussian_kde, entropy\n",
    "\n",
    "## Finding nicest dataset\n",
    "score_collection = []\n",
    "for file in [x for x in os.listdir(\"data/processed_data/\") if x not in [\".DS_Store\", \"README.md\"]]:\n",
    "    path = f\"data/processed_data/{file}/data.csv\"\n",
    "    # save figure\n",
    "    print(path)\n",
    "    plt.title(file)\n",
    "    pd.read_csv(path)[\"log_fitness\"].hist()\n",
    "    plt.savefig(f\"output/misc/{file}.jpg\")\n",
    "    plt.clf()\n",
    "\n",
    "    data = pd.read_csv(path)[\"log_fitness\"]\n",
    "\n",
    "    mu, sigma = np.mean(data), np.std(data)\n",
    "    kde = gaussian_kde(data)\n",
    "\n",
    "    # Evaluate on a fine grid\n",
    "    xs = np.linspace(data.min(), data.max(), 500)\n",
    "    p = kde(xs)\n",
    "    q = norm(loc=mu, scale=sigma).pdf(xs)\n",
    "\n",
    "    # Normalize to ensure both are proper distributions\n",
    "    p /= np.trapezoid(p, xs)\n",
    "    q /= np.trapezoid(q, xs)\n",
    "\n",
    "    kl_value = np.trapezoid(p * np.log(p / q), xs)\n",
    "\n",
    "    score_collection.append((kl_value, path))\n",
    "    # print(f'(kl-gauss) score: {kl_value}, path: {path}')\n",
    "\n",
    "# Note - (most gaussian) dataset:  UBE4B_MOUSE_Klevit2013-nscor_log2_ratio\n",
    "print()\n",
    "print(\"best pair: \", min(score_collection, key=lambda x: x[0]))\n",
    "print(\"array: \", np.array([x[0] for x in score_collection]))\n",
    "print(\"std: \", np.array([x[0] for x in score_collection]).std())\n",
    "\n",
    "\n",
    "# Todo\n",
    "# opt\n",
    "# - model quantization -> mps\n",
    "# - consider l1/l2\n",
    "# - what is dropout\n",
    "# - add early-stopping\n",
    "# - consider different optimziesr\n",
    "\n",
    "# results\n",
    "# - consider different optimziesr\n",
    "# - residuals\n",
    "# - adjust lr, number iters, num-embed-dim, kernel\n",
    "# - look into kmers + richer embeddings\n",
    "# -\n",
    "\n",
    "# facts\n",
    "# if accuracy decreases with more iterations\n",
    "# - overfitting\n",
    "# - learning rate too high\n",
    "# - without constraints like l1/l2/drop-out/early-stopping - could learn incorrect features\n",
    "# -\n",
    "\n",
    "# Q\n",
    "# - should I use MAE or RMSE how to pick one (MAE - linear but more robus, RMSE - aligns with GPR assumption)\n",
    "# - or should I use spearman/tau -> bc don't we actually care about ordering/rank more than precise viability values (we care about relative)\n",
    "# - Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
   ],
   "id": "743047871aea9e64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T16:56:26.830101Z",
     "start_time": "2025-09-05T16:56:26.409550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#esm2_t6_8M_UR50D -> R^320 embeddings\n",
    "\n",
    "esm_config = AutoConfig.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "esm_tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "esm_model = AutoModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "\n",
    "df = pd.read_csv(\"data/processed_data/UBE4B_MOUSE_Klevit2013-nscor_log2_ratio/data.csv\")\n",
    "df = df.rename(columns={\"seq\": \"sequence\", \"log_fitness\": \"viability\"})\n",
    "\n",
    "print(esm_config)\n",
    "print(df.shape)"
   ],
   "id": "b5f996ae18a19854",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EsmConfig {\n",
      "  \"architectures\": [\n",
      "    \"EsmForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"emb_layer_norm_before\": false,\n",
      "  \"esmfold_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 320,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1280,\n",
      "  \"is_folding_model\": false,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mask_token_id\": 32,\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"esm\",\n",
      "  \"num_attention_heads\": 20,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"rotary\",\n",
      "  \"token_dropout\": true,\n",
      "  \"transformers_version\": \"4.56.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_list\": null,\n",
      "  \"vocab_size\": 33\n",
      "}\n",
      "\n",
      "(32290, 3)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T16:56:27.673103Z",
     "start_time": "2025-09-05T16:56:27.650733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_file = \"cache/embeddings.pt\"\n",
    "\n",
    "if os.path.exists(embedding_file):\n",
    "    X = torch.load(embedding_file)\n",
    "else:\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for seq in df[\"sequence\"]:\n",
    "            tokenized_input = esm_tokenizer(seq, return_tensors=\"pt\")\n",
    "            output = esm_model(**tokenized_input)\n",
    "            seq_embedding = output.last_hidden_state.mean(dim=1).squeeze()\n",
    "            embeddings.append(seq_embedding)\n",
    "\n",
    "    X = torch.stack(embeddings)\n",
    "    torch.save(X, embedding_file)\n",
    "\n",
    "# Embedding-dim = 320\n",
    "\n",
    "# X = torch.load(embedding_file).half()\n",
    "# y = torch.tensor(df[\"viability\"].values, dtype=torch.float32).half()\n",
    "X = torch.load(embedding_file)\n",
    "y = torch.tensor(df[\"viability\"].values, dtype=torch.float32)\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=80)\n",
    "# X = pca.fit_transform(X.numpy())\n",
    "# print(\"Explained variance ratio:\", pca.explained_variance_ratio_.sum())"
   ],
   "id": "d4676f4472502f9f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T16:56:28.709249Z",
     "start_time": "2025-09-05T16:56:28.706121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gpytorch\n",
    "\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ],
   "id": "c8534c3f13573cab",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T18:44:41.069518Z",
     "start_time": "2025-09-05T16:56:30.289308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def run_mc_iteration(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        seed=None,\n",
    "        training_iterations=100,\n",
    "        lr=0.1,\n",
    "        patience=20,\n",
    "        tol=1e-4,\n",
    "):\n",
    "    # Split dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed\n",
    "    )\n",
    "\n",
    "    # Convert to tensors once\n",
    "    # X_train_tensor = torch.as_tensor(X_train, dtype=torch.float32)\n",
    "    # y_train_tensor = torch.as_tensor(y_train, dtype=torch.float32)\n",
    "    # X_test_tensor = torch.as_tensor(X_test, dtype=torch.float32)\n",
    "    # y_test_tensor = torch.as_tensor(y_test, dtype=torch.float32)\n",
    "    X_train_tensor = X_train\n",
    "    y_train_tensor = y_train\n",
    "    X_test_tensor = X_test\n",
    "    y_test_tensor = y_test\n",
    "\n",
    "    # Model + likelihood\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    gp_model = ExactGPModel(X_train_tensor, y_train_tensor, likelihood)\n",
    "\n",
    "    # Training\n",
    "    gp_model.train()\n",
    "    likelihood.train()\n",
    "    optimizer = torch.optim.Adam(gp_model.parameters(), lr=lr)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = gp_model(X_train_tensor)\n",
    "        loss = -mll(output, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Early stopping check\n",
    "        if loss.item() < best_loss - tol:\n",
    "            best_loss = loss.item()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at iteration {i}\")\n",
    "            break\n",
    "\n",
    "    # Evaluation\n",
    "    gp_model.eval()\n",
    "    likelihood.eval()\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        test_predictions = likelihood(gp_model(X_test_tensor))\n",
    "        test_means = test_predictions.mean.detach().numpy()\n",
    "\n",
    "    # Metrics\n",
    "    mae = mean_absolute_error(y_test_tensor, test_means)\n",
    "    mse = mean_squared_error(y_test_tensor, test_means)\n",
    "    rmse = np.sqrt(mse)\n",
    "    tau, p_value = kendalltau(y_test_tensor, test_means)\n",
    "\n",
    "    return {\n",
    "        \"mae\": mae,\n",
    "        \"mse\": mse,\n",
    "        \"rmse\": rmse,\n",
    "        \"tau\": tau,\n",
    "        \"p_value\": p_value,\n",
    "    }\n",
    "\n",
    "n_splits = 4\n",
    "test_size = 0.2\n",
    "training_iterations = 500\n",
    "lr = 0.001\n",
    "n_jobs=1\n",
    "\n",
    "\n",
    "# X = X.half()\n",
    "# y = y.half()\n",
    "\n",
    "results = Parallel(n_jobs=n_jobs, verbose=1)(\n",
    "    delayed(run_mc_iteration)(X, y, test_size=test_size, seed=i, training_iterations=training_iterations, lr=lr) for i in list(range(n_splits))\n",
    ")\n",
    "\n",
    "#1.5802\n",
    "# 6m 12s"
   ],
   "id": "f0c2b43b6db40686",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 108.2min finished\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T19:02:14.522138Z",
     "start_time": "2025-09-05T19:02:14.512527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "maes = [r[\"mae\"] for r in results]\n",
    "mses = [r[\"mse\"] for r in results]\n",
    "rmses = [r[\"rmse\"] for r in results]\n",
    "taus = [r[\"tau\"] for r in results]\n",
    "\n",
    "print(f\"sample size: {n_splits}, lr: {lr}, training_iterations: {training_iterations}\")\n",
    "print(f\"MAE: {np.mean(maes):.4f} ± {np.std(maes):.4f}\")\n",
    "print(f\"MSE: {np.mean(mses):.4f} ± {np.std(mses):.4f}\")\n",
    "print(f\"RMSE: {np.mean(rmses):.4f} ± {np.std(rmses):.4f}\")\n",
    "print(f\"Kendall's Tau: {np.mean(taus):.4f} ± {np.std(taus):.4f}\")"
   ],
   "id": "1671619c6d04ba9e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size: 4, lr: 0.001, training_iterations: 500\n",
      "MAE: 1.2840 ± 0.0151\n",
      "MSE: 2.6731 ± 0.0731\n",
      "RMSE: 1.6348 ± 0.0224\n",
      "Kendall's Tau: 0.4040 ± 0.0076\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T01:19:47.030368Z",
     "start_time": "2025-09-05T01:19:47.027201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## residual plots\n",
    "#\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "#\n",
    "# residuals = y_test - test_means\n",
    "#\n",
    "# # 1. Residuals vs Predicted\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# plt.scatter(test_means, residuals, alpha=0.6)\n",
    "# plt.axhline(0, color='red', linestyle='--')\n",
    "# plt.xlabel(\"Predicted values\")\n",
    "# plt.ylabel(\"Residuals\")\n",
    "# plt.title(\"Residuals vs Predicted\")\n",
    "# plt.show()\n",
    "#\n",
    "# # 2. Histogram of residuals\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# plt.hist(residuals, bins=30, alpha=0.7, edgecolor='k')\n",
    "# plt.xlabel(\"Residual\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.title(\"Histogram of Residuals\")\n",
    "# plt.show()\n",
    "#\n",
    "# # 3. QQ-plot of residuals (normality check)\n",
    "# import scipy.stats as stats\n",
    "# plt.figure(figsize=(6, 6))\n",
    "# stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "# plt.title(\"QQ Plot of Residuals\")\n",
    "# plt.show()\n",
    "#\n",
    "# # 4. Residuals vs each predictor (for multivariable regression)\n",
    "# if X_test_tensor.shape[1] <= 5:  # avoid plotting too many features\n",
    "#     X_test_np = X_test_tensor.numpy()\n",
    "#     for i in range(X_test_np.shape[1]):\n",
    "#         plt.figure(figsize=(6, 4))\n",
    "#         plt.scatter(X_test_np[:, i], residuals, alpha=0.6)\n",
    "#         plt.axhline(0, color='red', linestyle='--')\n",
    "#         plt.xlabel(f\"Feature {i}\")\n",
    "#         plt.ylabel(\"Residuals\")\n",
    "#         plt.title(f\"Residuals vs Feature {i}\")\n",
    "#         plt.show()\n"
   ],
   "id": "888ed4d014b02311",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T19:35:09.920650Z",
     "start_time": "2025-09-04T19:35:09.900887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## validate arbitrary training point\n",
    "# train_x = X_train_tensor[0].unsqueeze(0)\n",
    "# true_viability = y_train_tensor[0].item()\n",
    "#\n",
    "# with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "#     observed_pred = likelihood(gp_model(train_x))\n",
    "#     predicted_mean = observed_pred.mean.item()\n",
    "#     lower_bound, upper_bound = observed_pred.confidence_region()\n",
    "#\n",
    "# print(f\"True Viability: {true_viability}\")\n",
    "# print(f\"Predicted Viability: {predicted_mean}\")\n",
    "# print(f\"95% CI: [{lower_bound.item()}, {upper_bound.item()}]\")"
   ],
   "id": "ef86ba8cc6b815ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Viability: -3.128000020980835\n",
      "Predicted Viability: -2.9539124965667725\n",
      "95% CI: [-7.312348365783691, 1.4045231342315674]\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6d156380ec3db1b5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
